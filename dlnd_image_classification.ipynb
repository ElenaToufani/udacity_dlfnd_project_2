{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:32:45.840187+11:00",
     "start_time": "2017-03-30T00:32:39.448983"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:11, 15.0MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:33:03.225957+11:00",
     "start_time": "2017-03-30T00:32:58.356469"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 13 Max Value: 169\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd86c4005c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:33:06.870006+11:00",
     "start_time": "2017-03-30T00:33:06.737380"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:33:09.476004+11:00",
     "start_time": "2017-03-30T00:33:09.462609"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:33:30.931576+11:00",
     "start_time": "2017-03-30T00:33:12.804601"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:33:33.854410+11:00",
     "start_time": "2017-03-30T00:33:33.753709"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:34:30.315524+11:00",
     "start_time": "2017-03-30T00:34:30.161567"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input, 4D tensor -> [n_obs x 32 x 32 x 3]\n",
    "    \n",
    "    Dimension of tensor returned is based on the images passed in\n",
    "        * shape[None... ->  batch size. None allows for the row count to be\n",
    "                            dynamic, due to batches potentially having a\n",
    "                            different number of observations\n",
    "        * image_shape[n] -> provides the dimensions for the image\n",
    "                         -> [0] = y (32)\n",
    "                         -> [1] = x (32)\n",
    "                         -> [2] = depth (colour channel count = 3 = RGB)\n",
    "                         \n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,\n",
    "                          shape=[None,\n",
    "                                 image_shape[0],\n",
    "                                 image_shape[1],\n",
    "                                 image_shape[2]],\n",
    "                          name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:23.165136+11:00",
     "start_time": "2017-03-30T01:31:22.714461"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=eBbEDRsCmv4\n",
    "# Video on setting scope names for tensorboard\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, scope_name=\"conv\"):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope_name):\n",
    "        # set locals\n",
    "        # ------------------------------------------------------------------------\n",
    "        # coeffecient control var(s)\n",
    "        layer_depth = x_tensor.get_shape()[3].value\n",
    "        weight_shape = [*conv_ksize, layer_depth, conv_num_outputs]\n",
    "        bias_shape = conv_num_outputs\n",
    "\n",
    "        # convolution control var(s)\n",
    "        padding = \"SAME\"\n",
    "        conv_strides = [1, *conv_strides, 1]\n",
    "\n",
    "        # max pool control vars\n",
    "        ksize = [1, *pool_ksize, 1]\n",
    "        pool_strides = [1, *pool_strides, 1]\n",
    "\n",
    "\n",
    "        # set coeffecients\n",
    "        # ------------------------------------------------------------------------\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/truncated_normal\n",
    "        # https://www.tensorflow.org/programmers_guide/variable_scope\n",
    "        weights = tf.Variable(tf.truncated_normal(weight_shape, stddev=0.1),\n",
    "                              tf.float32,\n",
    "                              name=\"weights\")\n",
    "        bias = tf.Variable(tf.zeros(bias_shape, tf.float32, name=\"bias\"))\n",
    "\n",
    "\n",
    "        # set nodes\n",
    "        # ------------------------------------------------------------------------\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/relu\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\n",
    "        convolutions = tf.nn.conv2d(x_tensor, weights, conv_strides, padding, name=\"conv\")\n",
    "        activations = tf.nn.relu(convolutions + bias, name=\"relu\")\n",
    "        pool = tf.nn.max_pool(activations, ksize, pool_strides, padding, name=\"pool\")\n",
    "\n",
    "        return pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T00:35:48.866223+11:00",
     "start_time": "2017-03-30T00:35:43.653764"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tfcl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:26.414881+11:00",
     "start_time": "2017-03-30T01:31:26.399930"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten\n",
    "    return tfcl.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:28.773561+11:00",
     "start_time": "2017-03-30T01:31:28.685757"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs, scope_name=\"fc\"):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\n",
    "    with tf.name_scope(scope_name):\n",
    "        return tfcl.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:30.989717+11:00",
     "start_time": "2017-03-30T01:31:30.933040"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs, scope_name=\"output\"):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\n",
    "    # activation_fn: activation function, set to None to skip it and maintain a linear activation.\n",
    "    with tf.name_scope(scope_name):\n",
    "        return tfcl.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:33.598382+11:00",
     "start_time": "2017-03-30T01:31:32.890521"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "# https://cs231n.github.io/convolutional-networks/\n",
    "# https://cs231n.github.io/convolutional-networks/#layers\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \n",
    "    Internals:\n",
    "        - conv1 = [32, 32, 3]\n",
    "        - conv2 = []\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_1 = conv2d_maxpool(x_tensor=x, \n",
    "                            conv_num_outputs=32,\n",
    "                            conv_ksize=(3, 3), \n",
    "                            conv_strides=(1, 1), \n",
    "                            pool_ksize=(2, 2), \n",
    "                            pool_strides=(2, 2),\n",
    "                            scope_name=\"conv_1\")\n",
    "    \n",
    "    conv_2 = conv2d_maxpool(x_tensor=conv_1,\n",
    "                            conv_num_outputs=196,\n",
    "                            conv_ksize=(3, 3), \n",
    "                            conv_strides=(1, 1), \n",
    "                            pool_ksize=(2, 2), \n",
    "                            pool_strides=(2, 2),\n",
    "                            scope_name=\"conv_2\")\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv_2)\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/dropout\n",
    "    fc_1 = fully_conn(flat, 512, scope_name=\"fc_1\")\n",
    "    fc_1 = tf.nn.dropout(fc_1, keep_prob, name=\"do_1\")\n",
    "    \n",
    "    fc_2 = fully_conn(fc_1, 512, scope_name=\"fc_2\")\n",
    "    fc_2 = tf.nn.dropout(fc_2, keep_prob, name=\"do_2\")\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x = output(fc_2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:36.718713+11:00",
     "start_time": "2017-03-30T01:31:36.568858"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, \n",
    "                feed_dict={x: feature_batch, \n",
    "                           y: label_batch, \n",
    "                           keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:31:38.176901+11:00",
     "start_time": "2017-03-30T01:31:38.164726"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    keep_prob_for_stats = 1.\n",
    "    cost_stat = session.run(cost,\n",
    "                            feed_dict={x: feature_batch,\n",
    "                                       y: label_batch, \n",
    "                                       keep_prob: keep_prob_for_stats})\n",
    "    accuracy_stat = session.run(accuracy,\n",
    "                                feed_dict={x: valid_features, \n",
    "                                           y: valid_labels, \n",
    "                                           keep_prob: keep_prob_for_stats})\n",
    "    print(\"{} Cost: {:.5f}\\tAccuracy: {:.5f}\".format(datetime.now(),\n",
    "                                                               cost_stat, \n",
    "                                                               accuracy_stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:41:57.269797+11:00",
     "start_time": "2017-03-30T01:41:57.256472"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:48:39.798470+11:00",
     "start_time": "2017-03-30T01:42:02.584030"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  2017-03-30 10:58:00.115211 Cost: 2.11637\tAccuracy: 0.30980\n",
      "Epoch  2, CIFAR-10 Batch 1:  2017-03-30 10:58:04.043689 Cost: 1.70745\tAccuracy: 0.41680\n",
      "Epoch  3, CIFAR-10 Batch 1:  2017-03-30 10:58:07.973543 Cost: 1.32539\tAccuracy: 0.47300\n",
      "Epoch  4, CIFAR-10 Batch 1:  2017-03-30 10:58:11.901729 Cost: 1.03046\tAccuracy: 0.51800\n",
      "Epoch  5, CIFAR-10 Batch 1:  2017-03-30 10:58:15.859611 Cost: 0.83282\tAccuracy: 0.54620\n",
      "Epoch  6, CIFAR-10 Batch 1:  2017-03-30 10:58:19.789084 Cost: 0.66248\tAccuracy: 0.56300\n",
      "Epoch  7, CIFAR-10 Batch 1:  2017-03-30 10:58:23.723323 Cost: 0.49221\tAccuracy: 0.55780\n",
      "Epoch  8, CIFAR-10 Batch 1:  2017-03-30 10:58:27.636340 Cost: 0.36651\tAccuracy: 0.55440\n",
      "Epoch  9, CIFAR-10 Batch 1:  2017-03-30 10:58:31.536915 Cost: 0.26037\tAccuracy: 0.58000\n",
      "Epoch 10, CIFAR-10 Batch 1:  2017-03-30 10:58:35.430823 Cost: 0.16317\tAccuracy: 0.60820\n",
      "Epoch 11, CIFAR-10 Batch 1:  2017-03-30 10:58:39.335904 Cost: 0.12071\tAccuracy: 0.60180\n",
      "Epoch 12, CIFAR-10 Batch 1:  2017-03-30 10:58:43.232284 Cost: 0.07359\tAccuracy: 0.60720\n",
      "Epoch 13, CIFAR-10 Batch 1:  2017-03-30 10:58:47.130358 Cost: 0.07213\tAccuracy: 0.58860\n",
      "Epoch 14, CIFAR-10 Batch 1:  2017-03-30 10:58:51.066973 Cost: 0.05071\tAccuracy: 0.58880\n",
      "Epoch 15, CIFAR-10 Batch 1:  2017-03-30 10:58:55.018475 Cost: 0.03468\tAccuracy: 0.61960\n",
      "Epoch 16, CIFAR-10 Batch 1:  2017-03-30 10:58:58.978660 Cost: 0.02406\tAccuracy: 0.59280\n",
      "Epoch 17, CIFAR-10 Batch 1:  2017-03-30 10:59:02.934538 Cost: 0.01550\tAccuracy: 0.61060\n",
      "Epoch 18, CIFAR-10 Batch 1:  2017-03-30 10:59:06.885690 Cost: 0.00932\tAccuracy: 0.59760\n",
      "Epoch 19, CIFAR-10 Batch 1:  2017-03-30 10:59:10.821221 Cost: 0.00979\tAccuracy: 0.61240\n",
      "Epoch 20, CIFAR-10 Batch 1:  2017-03-30 10:59:14.778616 Cost: 0.00959\tAccuracy: 0.60400\n",
      "Epoch 21, CIFAR-10 Batch 1:  2017-03-30 10:59:18.739684 Cost: 0.00574\tAccuracy: 0.61200\n",
      "Epoch 22, CIFAR-10 Batch 1:  2017-03-30 10:59:22.683966 Cost: 0.00384\tAccuracy: 0.59000\n",
      "Epoch 23, CIFAR-10 Batch 1:  2017-03-30 10:59:26.641965 Cost: 0.00373\tAccuracy: 0.58840\n",
      "Epoch 24, CIFAR-10 Batch 1:  2017-03-30 10:59:30.599347 Cost: 0.00267\tAccuracy: 0.60980\n",
      "Epoch 25, CIFAR-10 Batch 1:  2017-03-30 10:59:34.551542 Cost: 0.00183\tAccuracy: 0.61340\n",
      "Epoch 26, CIFAR-10 Batch 1:  2017-03-30 10:59:38.505761 Cost: 0.00195\tAccuracy: 0.60880\n",
      "Epoch 27, CIFAR-10 Batch 1:  2017-03-30 10:59:42.461184 Cost: 0.00105\tAccuracy: 0.59820\n",
      "Epoch 28, CIFAR-10 Batch 1:  2017-03-30 10:59:46.395995 Cost: 0.00062\tAccuracy: 0.61700\n",
      "Epoch 29, CIFAR-10 Batch 1:  2017-03-30 10:59:50.324291 Cost: 0.00058\tAccuracy: 0.59320\n",
      "Epoch 30, CIFAR-10 Batch 1:  2017-03-30 10:59:54.267170 Cost: 0.00105\tAccuracy: 0.59860\n",
      "Epoch 31, CIFAR-10 Batch 1:  2017-03-30 10:59:58.226204 Cost: 0.00043\tAccuracy: 0.60600\n",
      "Epoch 32, CIFAR-10 Batch 1:  2017-03-30 11:00:02.176586 Cost: 0.00009\tAccuracy: 0.61920\n",
      "Epoch 33, CIFAR-10 Batch 1:  2017-03-30 11:00:06.136351 Cost: 0.00031\tAccuracy: 0.59640\n",
      "Epoch 34, CIFAR-10 Batch 1:  2017-03-30 11:00:10.095861 Cost: 0.00026\tAccuracy: 0.61640\n",
      "Epoch 35, CIFAR-10 Batch 1:  2017-03-30 11:00:14.047017 Cost: 0.00023\tAccuracy: 0.59900\n",
      "Epoch 36, CIFAR-10 Batch 1:  2017-03-30 11:00:18.002192 Cost: 0.00040\tAccuracy: 0.60940\n",
      "Epoch 37, CIFAR-10 Batch 1:  2017-03-30 11:00:21.959608 Cost: 0.00049\tAccuracy: 0.60080\n",
      "Epoch 38, CIFAR-10 Batch 1:  2017-03-30 11:00:25.921087 Cost: 0.00025\tAccuracy: 0.61160\n",
      "Epoch 39, CIFAR-10 Batch 1:  2017-03-30 11:00:29.874530 Cost: 0.00188\tAccuracy: 0.60800\n",
      "Epoch 40, CIFAR-10 Batch 1:  2017-03-30 11:00:33.804380 Cost: 0.00043\tAccuracy: 0.60840\n",
      "Epoch 41, CIFAR-10 Batch 1:  2017-03-30 11:00:37.711235 Cost: 0.00044\tAccuracy: 0.61280\n",
      "Epoch 42, CIFAR-10 Batch 1:  2017-03-30 11:00:41.616329 Cost: 0.00018\tAccuracy: 0.61820\n",
      "Epoch 43, CIFAR-10 Batch 1:  2017-03-30 11:00:45.525570 Cost: 0.00106\tAccuracy: 0.61360\n",
      "Epoch 44, CIFAR-10 Batch 1:  2017-03-30 11:00:49.427541 Cost: 0.00040\tAccuracy: 0.60480\n",
      "Epoch 45, CIFAR-10 Batch 1:  2017-03-30 11:00:53.334148 Cost: 0.00010\tAccuracy: 0.61540\n",
      "Epoch 46, CIFAR-10 Batch 1:  2017-03-30 11:00:57.241412 Cost: 0.00019\tAccuracy: 0.61320\n",
      "Epoch 47, CIFAR-10 Batch 1:  2017-03-30 11:01:01.161667 Cost: 0.00008\tAccuracy: 0.61100\n",
      "Epoch 48, CIFAR-10 Batch 1:  2017-03-30 11:01:05.086942 Cost: 0.00005\tAccuracy: 0.61820\n",
      "Epoch 49, CIFAR-10 Batch 1:  2017-03-30 11:01:08.989266 Cost: 0.00013\tAccuracy: 0.60520\n",
      "Epoch 50, CIFAR-10 Batch 1:  2017-03-30 11:01:12.916015 Cost: 0.00006\tAccuracy: 0.62220\n",
      "Epoch 51, CIFAR-10 Batch 1:  2017-03-30 11:01:16.842796 Cost: 0.00003\tAccuracy: 0.61280\n",
      "Epoch 52, CIFAR-10 Batch 1:  2017-03-30 11:01:20.754801 Cost: 0.00005\tAccuracy: 0.61960\n",
      "Epoch 53, CIFAR-10 Batch 1:  2017-03-30 11:01:24.686203 Cost: 0.00014\tAccuracy: 0.60840\n",
      "Epoch 54, CIFAR-10 Batch 1:  2017-03-30 11:01:28.618164 Cost: 0.00012\tAccuracy: 0.60280\n",
      "Epoch 55, CIFAR-10 Batch 1:  2017-03-30 11:01:32.547173 Cost: 0.00007\tAccuracy: 0.60520\n",
      "Epoch 56, CIFAR-10 Batch 1:  2017-03-30 11:01:36.484431 Cost: 0.00032\tAccuracy: 0.59960\n",
      "Epoch 57, CIFAR-10 Batch 1:  2017-03-30 11:01:40.425508 Cost: 0.00019\tAccuracy: 0.60640\n",
      "Epoch 58, CIFAR-10 Batch 1:  2017-03-30 11:01:44.357909 Cost: 0.00021\tAccuracy: 0.61040\n",
      "Epoch 59, CIFAR-10 Batch 1:  2017-03-30 11:01:48.306429 Cost: 0.00015\tAccuracy: 0.61260\n",
      "Epoch 60, CIFAR-10 Batch 1:  2017-03-30 11:01:52.244263 Cost: 0.00010\tAccuracy: 0.59940\n",
      "Epoch 61, CIFAR-10 Batch 1:  2017-03-30 11:01:56.191412 Cost: 0.00006\tAccuracy: 0.61200\n",
      "Epoch 62, CIFAR-10 Batch 1:  2017-03-30 11:02:00.138192 Cost: 0.00046\tAccuracy: 0.59200\n",
      "Epoch 63, CIFAR-10 Batch 1:  2017-03-30 11:02:04.067029 Cost: 0.00004\tAccuracy: 0.59820\n",
      "Epoch 64, CIFAR-10 Batch 1:  2017-03-30 11:02:08.012044 Cost: 0.00005\tAccuracy: 0.61500\n",
      "Epoch 65, CIFAR-10 Batch 1:  2017-03-30 11:02:11.976016 Cost: 0.00007\tAccuracy: 0.62380\n",
      "Epoch 66, CIFAR-10 Batch 1:  2017-03-30 11:02:15.928209 Cost: 0.00008\tAccuracy: 0.60900\n",
      "Epoch 67, CIFAR-10 Batch 1:  2017-03-30 11:02:19.887444 Cost: 0.00418\tAccuracy: 0.61040\n",
      "Epoch 68, CIFAR-10 Batch 1:  2017-03-30 11:02:23.832479 Cost: 0.00017\tAccuracy: 0.60220\n",
      "Epoch 69, CIFAR-10 Batch 1:  2017-03-30 11:02:27.783042 Cost: 0.00012\tAccuracy: 0.60880\n",
      "Epoch 70, CIFAR-10 Batch 1:  2017-03-30 11:02:31.743753 Cost: 0.00007\tAccuracy: 0.60580\n",
      "Epoch 71, CIFAR-10 Batch 1:  2017-03-30 11:02:35.695295 Cost: 0.00009\tAccuracy: 0.60500\n",
      "Epoch 72, CIFAR-10 Batch 1:  2017-03-30 11:02:39.640664 Cost: 0.00012\tAccuracy: 0.61120\n",
      "Epoch 73, CIFAR-10 Batch 1:  2017-03-30 11:02:43.582336 Cost: 0.00022\tAccuracy: 0.60720\n",
      "Epoch 74, CIFAR-10 Batch 1:  2017-03-30 11:02:47.528370 Cost: 0.00003\tAccuracy: 0.60660\n",
      "Epoch 75, CIFAR-10 Batch 1:  2017-03-30 11:02:51.468525 Cost: 0.00005\tAccuracy: 0.61000\n",
      "Epoch 76, CIFAR-10 Batch 1:  2017-03-30 11:02:55.409368 Cost: 0.00105\tAccuracy: 0.60820\n",
      "Epoch 77, CIFAR-10 Batch 1:  2017-03-30 11:02:59.356199 Cost: 0.00003\tAccuracy: 0.60980\n",
      "Epoch 78, CIFAR-10 Batch 1:  2017-03-30 11:03:03.303677 Cost: 0.00001\tAccuracy: 0.60660\n",
      "Epoch 79, CIFAR-10 Batch 1:  2017-03-30 11:03:07.251324 Cost: 0.00003\tAccuracy: 0.60880\n",
      "Epoch 80, CIFAR-10 Batch 1:  2017-03-30 11:03:11.197674 Cost: 0.00001\tAccuracy: 0.60820\n",
      "Epoch 81, CIFAR-10 Batch 1:  2017-03-30 11:03:15.155783 Cost: 0.00001\tAccuracy: 0.60620\n",
      "Epoch 82, CIFAR-10 Batch 1:  2017-03-30 11:03:19.094772 Cost: 0.00001\tAccuracy: 0.61080\n",
      "Epoch 83, CIFAR-10 Batch 1:  2017-03-30 11:03:23.031265 Cost: 0.00001\tAccuracy: 0.61800\n",
      "Epoch 84, CIFAR-10 Batch 1:  2017-03-30 11:03:26.986889 Cost: 0.00002\tAccuracy: 0.60920\n",
      "Epoch 85, CIFAR-10 Batch 1:  2017-03-30 11:03:30.941752 Cost: 0.00002\tAccuracy: 0.61280\n",
      "Epoch 86, CIFAR-10 Batch 1:  2017-03-30 11:03:34.897037 Cost: 0.00266\tAccuracy: 0.61140\n",
      "Epoch 87, CIFAR-10 Batch 1:  2017-03-30 11:03:38.858599 Cost: 0.00005\tAccuracy: 0.60940\n",
      "Epoch 88, CIFAR-10 Batch 1:  2017-03-30 11:03:42.808992 Cost: 0.00001\tAccuracy: 0.61920\n",
      "Epoch 89, CIFAR-10 Batch 1:  2017-03-30 11:03:46.765485 Cost: 0.00004\tAccuracy: 0.61740\n",
      "Epoch 90, CIFAR-10 Batch 1:  2017-03-30 11:03:50.715428 Cost: 0.00005\tAccuracy: 0.60360\n",
      "Epoch 91, CIFAR-10 Batch 1:  2017-03-30 11:03:54.674878 Cost: 0.00004\tAccuracy: 0.61640\n",
      "Epoch 92, CIFAR-10 Batch 1:  2017-03-30 11:03:58.624570 Cost: 0.00002\tAccuracy: 0.61400\n",
      "Epoch 93, CIFAR-10 Batch 1:  2017-03-30 11:04:02.589451 Cost: 0.00001\tAccuracy: 0.60340\n",
      "Epoch 94, CIFAR-10 Batch 1:  2017-03-30 11:04:06.514110 Cost: 0.00002\tAccuracy: 0.60740\n",
      "Epoch 95, CIFAR-10 Batch 1:  2017-03-30 11:04:10.420065 Cost: 0.00003\tAccuracy: 0.61560\n",
      "Epoch 96, CIFAR-10 Batch 1:  2017-03-30 11:04:14.348126 Cost: 0.00000\tAccuracy: 0.61400\n",
      "Epoch 97, CIFAR-10 Batch 1:  2017-03-30 11:04:18.262311 Cost: 0.00009\tAccuracy: 0.60760\n",
      "Epoch 98, CIFAR-10 Batch 1:  2017-03-30 11:04:22.192323 Cost: 0.00002\tAccuracy: 0.59700\n",
      "Epoch 99, CIFAR-10 Batch 1:  2017-03-30 11:04:26.120017 Cost: 0.00005\tAccuracy: 0.60660\n",
      "Epoch 100, CIFAR-10 Batch 1:  2017-03-30 11:04:30.036018 Cost: 0.00006\tAccuracy: 0.60720\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-30T01:27:55.614678+11:00",
     "start_time": "2017-03-30T01:24:11.493609"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  2017-03-30 11:04:34.079515 Cost: 2.06709\tAccuracy: 0.31040\n",
      "Epoch  1, CIFAR-10 Batch 2:  2017-03-30 11:04:38.002438 Cost: 1.43076\tAccuracy: 0.41260\n",
      "Epoch  1, CIFAR-10 Batch 3:  2017-03-30 11:04:41.912073 Cost: 1.21854\tAccuracy: 0.46900\n",
      "Epoch  1, CIFAR-10 Batch 4:  2017-03-30 11:04:45.861012 Cost: 1.32382\tAccuracy: 0.51600\n",
      "Epoch  1, CIFAR-10 Batch 5:  2017-03-30 11:04:49.807611 Cost: 1.22135\tAccuracy: 0.54360\n",
      "Epoch  2, CIFAR-10 Batch 1:  2017-03-30 11:04:53.753475 Cost: 1.34290\tAccuracy: 0.56140\n",
      "Epoch  2, CIFAR-10 Batch 2:  2017-03-30 11:04:57.699062 Cost: 0.85420\tAccuracy: 0.57320\n",
      "Epoch  2, CIFAR-10 Batch 3:  2017-03-30 11:05:01.647009 Cost: 0.68395\tAccuracy: 0.58120\n",
      "Epoch  2, CIFAR-10 Batch 4:  2017-03-30 11:05:05.602446 Cost: 0.81972\tAccuracy: 0.60660\n",
      "Epoch  2, CIFAR-10 Batch 5:  2017-03-30 11:05:09.542358 Cost: 0.75457\tAccuracy: 0.60720\n",
      "Epoch  3, CIFAR-10 Batch 1:  2017-03-30 11:05:13.496279 Cost: 0.85121\tAccuracy: 0.64000\n",
      "Epoch  3, CIFAR-10 Batch 2:  2017-03-30 11:05:17.445689 Cost: 0.51823\tAccuracy: 0.62320\n",
      "Epoch  3, CIFAR-10 Batch 3:  2017-03-30 11:05:21.401470 Cost: 0.40426\tAccuracy: 0.65200\n",
      "Epoch  3, CIFAR-10 Batch 4:  2017-03-30 11:05:25.337603 Cost: 0.50455\tAccuracy: 0.65540\n",
      "Epoch  3, CIFAR-10 Batch 5:  2017-03-30 11:05:29.276507 Cost: 0.39841\tAccuracy: 0.66540\n",
      "Epoch  4, CIFAR-10 Batch 1:  2017-03-30 11:05:33.225855 Cost: 0.51236\tAccuracy: 0.67820\n",
      "Epoch  4, CIFAR-10 Batch 2:  2017-03-30 11:05:37.151697 Cost: 0.29252\tAccuracy: 0.64520\n",
      "Epoch  4, CIFAR-10 Batch 3:  2017-03-30 11:05:41.106800 Cost: 0.23340\tAccuracy: 0.68880\n",
      "Epoch  4, CIFAR-10 Batch 4:  2017-03-30 11:05:45.061145 Cost: 0.30916\tAccuracy: 0.66320\n",
      "Epoch  4, CIFAR-10 Batch 5:  2017-03-30 11:05:48.973368 Cost: 0.23510\tAccuracy: 0.66360\n",
      "Epoch  5, CIFAR-10 Batch 1:  2017-03-30 11:05:52.890192 Cost: 0.29519\tAccuracy: 0.66360\n",
      "Epoch  5, CIFAR-10 Batch 2:  2017-03-30 11:05:56.806011 Cost: 0.14689\tAccuracy: 0.67320\n",
      "Epoch  5, CIFAR-10 Batch 3:  2017-03-30 11:06:00.744259 Cost: 0.14422\tAccuracy: 0.68140\n",
      "Epoch  5, CIFAR-10 Batch 4:  2017-03-30 11:06:04.670625 Cost: 0.19242\tAccuracy: 0.69420\n",
      "Epoch  5, CIFAR-10 Batch 5:  2017-03-30 11:06:08.633166 Cost: 0.13625\tAccuracy: 0.68720\n",
      "Epoch  6, CIFAR-10 Batch 1:  2017-03-30 11:06:12.603854 Cost: 0.18957\tAccuracy: 0.68280\n",
      "Epoch  6, CIFAR-10 Batch 2:  2017-03-30 11:06:16.573208 Cost: 0.07782\tAccuracy: 0.69120\n",
      "Epoch  6, CIFAR-10 Batch 3:  2017-03-30 11:06:20.535314 Cost: 0.08581\tAccuracy: 0.68640\n",
      "Epoch  6, CIFAR-10 Batch 4:  2017-03-30 11:06:24.496846 Cost: 0.10816\tAccuracy: 0.70400\n",
      "Epoch  6, CIFAR-10 Batch 5:  2017-03-30 11:06:28.435549 Cost: 0.06381\tAccuracy: 0.69400\n",
      "Epoch  7, CIFAR-10 Batch 1:  2017-03-30 11:06:32.397686 Cost: 0.10979\tAccuracy: 0.70280\n",
      "Epoch  7, CIFAR-10 Batch 2:  2017-03-30 11:06:36.351671 Cost: 0.04259\tAccuracy: 0.70460\n",
      "Epoch  7, CIFAR-10 Batch 3:  2017-03-30 11:06:40.296939 Cost: 0.05247\tAccuracy: 0.69000\n",
      "Epoch  7, CIFAR-10 Batch 4:  2017-03-30 11:06:44.257654 Cost: 0.05547\tAccuracy: 0.70520\n",
      "Epoch  7, CIFAR-10 Batch 5:  2017-03-30 11:06:48.222390 Cost: 0.03757\tAccuracy: 0.67780\n",
      "Epoch  8, CIFAR-10 Batch 1:  2017-03-30 11:06:52.185297 Cost: 0.06295\tAccuracy: 0.69380\n",
      "Epoch  8, CIFAR-10 Batch 2:  2017-03-30 11:06:56.149752 Cost: 0.03195\tAccuracy: 0.69980\n",
      "Epoch  8, CIFAR-10 Batch 3:  2017-03-30 11:07:00.075009 Cost: 0.04532\tAccuracy: 0.69240\n",
      "Epoch  8, CIFAR-10 Batch 4:  2017-03-30 11:07:04.006806 Cost: 0.03031\tAccuracy: 0.70820\n",
      "Epoch  8, CIFAR-10 Batch 5:  2017-03-30 11:07:07.933848 Cost: 0.02226\tAccuracy: 0.70120\n",
      "Epoch  9, CIFAR-10 Batch 1:  2017-03-30 11:07:11.868322 Cost: 0.06480\tAccuracy: 0.68680\n",
      "Epoch  9, CIFAR-10 Batch 2:  2017-03-30 11:07:15.799432 Cost: 0.00949\tAccuracy: 0.71020\n",
      "Epoch  9, CIFAR-10 Batch 3:  2017-03-30 11:07:19.726609 Cost: 0.03178\tAccuracy: 0.68300\n",
      "Epoch  9, CIFAR-10 Batch 4:  2017-03-30 11:07:23.650786 Cost: 0.02054\tAccuracy: 0.69400\n",
      "Epoch  9, CIFAR-10 Batch 5:  2017-03-30 11:07:27.576328 Cost: 0.01589\tAccuracy: 0.69460\n",
      "Epoch 10, CIFAR-10 Batch 1:  2017-03-30 11:07:31.519542 Cost: 0.02444\tAccuracy: 0.68280\n",
      "Epoch 10, CIFAR-10 Batch 2:  2017-03-30 11:07:35.481472 Cost: 0.00898\tAccuracy: 0.69680\n",
      "Epoch 10, CIFAR-10 Batch 3:  2017-03-30 11:07:39.444408 Cost: 0.01411\tAccuracy: 0.69140\n",
      "Epoch 10, CIFAR-10 Batch 4:  2017-03-30 11:07:43.405266 Cost: 0.01830\tAccuracy: 0.69300\n",
      "Epoch 10, CIFAR-10 Batch 5:  2017-03-30 11:07:47.322039 Cost: 0.00718\tAccuracy: 0.70820\n",
      "Epoch 11, CIFAR-10 Batch 1:  2017-03-30 11:07:51.227112 Cost: 0.02016\tAccuracy: 0.68240\n",
      "Epoch 11, CIFAR-10 Batch 2:  2017-03-30 11:07:55.130278 Cost: 0.02691\tAccuracy: 0.67120\n",
      "Epoch 11, CIFAR-10 Batch 3:  2017-03-30 11:07:59.029711 Cost: 0.00843\tAccuracy: 0.69600\n",
      "Epoch 11, CIFAR-10 Batch 4:  2017-03-30 11:08:02.938498 Cost: 0.01325\tAccuracy: 0.67640\n",
      "Epoch 11, CIFAR-10 Batch 5:  2017-03-30 11:08:06.842406 Cost: 0.00476\tAccuracy: 0.71260\n",
      "Epoch 12, CIFAR-10 Batch 1:  2017-03-30 11:08:10.746134 Cost: 0.01089\tAccuracy: 0.68660\n",
      "Epoch 12, CIFAR-10 Batch 2:  2017-03-30 11:08:14.653860 Cost: 0.00688\tAccuracy: 0.66960\n",
      "Epoch 12, CIFAR-10 Batch 3:  2017-03-30 11:08:18.556085 Cost: 0.00477\tAccuracy: 0.70420\n",
      "Epoch 12, CIFAR-10 Batch 4:  2017-03-30 11:08:22.463909 Cost: 0.01065\tAccuracy: 0.68620\n",
      "Epoch 12, CIFAR-10 Batch 5:  2017-03-30 11:08:26.363955 Cost: 0.00248\tAccuracy: 0.71600\n",
      "Epoch 13, CIFAR-10 Batch 1:  2017-03-30 11:08:30.270701 Cost: 0.01054\tAccuracy: 0.67020\n",
      "Epoch 13, CIFAR-10 Batch 2:  2017-03-30 11:08:34.169545 Cost: 0.01224\tAccuracy: 0.67200\n",
      "Epoch 13, CIFAR-10 Batch 3:  2017-03-30 11:08:38.076455 Cost: 0.00236\tAccuracy: 0.70840\n",
      "Epoch 13, CIFAR-10 Batch 4:  2017-03-30 11:08:41.978848 Cost: 0.00307\tAccuracy: 0.69800\n",
      "Epoch 13, CIFAR-10 Batch 5:  2017-03-30 11:08:45.877925 Cost: 0.00354\tAccuracy: 0.70580\n",
      "Epoch 14, CIFAR-10 Batch 1:  2017-03-30 11:08:49.788401 Cost: 0.00652\tAccuracy: 0.69580\n",
      "Epoch 14, CIFAR-10 Batch 2:  2017-03-30 11:08:53.686956 Cost: 0.00244\tAccuracy: 0.68820\n",
      "Epoch 14, CIFAR-10 Batch 3:  2017-03-30 11:08:57.588021 Cost: 0.01115\tAccuracy: 0.70620\n",
      "Epoch 14, CIFAR-10 Batch 4:  2017-03-30 11:09:01.490960 Cost: 0.00290\tAccuracy: 0.70180\n",
      "Epoch 14, CIFAR-10 Batch 5:  2017-03-30 11:09:05.390152 Cost: 0.00157\tAccuracy: 0.70840\n",
      "Epoch 15, CIFAR-10 Batch 1:  2017-03-30 11:09:09.319939 Cost: 0.00374\tAccuracy: 0.68460\n",
      "Epoch 15, CIFAR-10 Batch 2:  2017-03-30 11:09:13.252504 Cost: 0.00419\tAccuracy: 0.69220\n",
      "Epoch 15, CIFAR-10 Batch 3:  2017-03-30 11:09:17.181652 Cost: 0.00085\tAccuracy: 0.70020\n",
      "Epoch 15, CIFAR-10 Batch 4:  2017-03-30 11:09:21.080352 Cost: 0.00211\tAccuracy: 0.70020\n",
      "Epoch 15, CIFAR-10 Batch 5:  2017-03-30 11:09:24.982587 Cost: 0.00090\tAccuracy: 0.70800\n",
      "Epoch 16, CIFAR-10 Batch 1:  2017-03-30 11:09:28.890609 Cost: 0.00316\tAccuracy: 0.69020\n",
      "Epoch 16, CIFAR-10 Batch 2:  2017-03-30 11:09:32.796618 Cost: 0.00147\tAccuracy: 0.69520\n",
      "Epoch 16, CIFAR-10 Batch 3:  2017-03-30 11:09:36.723265 Cost: 0.00036\tAccuracy: 0.70000\n",
      "Epoch 16, CIFAR-10 Batch 4:  2017-03-30 11:09:40.644163 Cost: 0.00195\tAccuracy: 0.70660\n",
      "Epoch 16, CIFAR-10 Batch 5:  2017-03-30 11:09:44.550980 Cost: 0.00073\tAccuracy: 0.70440\n",
      "Epoch 17, CIFAR-10 Batch 1:  2017-03-30 11:09:48.458649 Cost: 0.00128\tAccuracy: 0.70320\n",
      "Epoch 17, CIFAR-10 Batch 2:  2017-03-30 11:09:52.381636 Cost: 0.00099\tAccuracy: 0.70940\n",
      "Epoch 17, CIFAR-10 Batch 3:  2017-03-30 11:09:56.291246 Cost: 0.00121\tAccuracy: 0.70640\n",
      "Epoch 17, CIFAR-10 Batch 4:  2017-03-30 11:10:00.206580 Cost: 0.00107\tAccuracy: 0.71240\n",
      "Epoch 17, CIFAR-10 Batch 5:  2017-03-30 11:10:04.126899 Cost: 0.00104\tAccuracy: 0.70240\n",
      "Epoch 18, CIFAR-10 Batch 1:  2017-03-30 11:10:08.037677 Cost: 0.00239\tAccuracy: 0.69240\n",
      "Epoch 18, CIFAR-10 Batch 2:  2017-03-30 11:10:11.943470 Cost: 0.00052\tAccuracy: 0.70620\n",
      "Epoch 18, CIFAR-10 Batch 3:  2017-03-30 11:10:15.880387 Cost: 0.00011\tAccuracy: 0.70700\n",
      "Epoch 18, CIFAR-10 Batch 4:  2017-03-30 11:10:19.794015 Cost: 0.00189\tAccuracy: 0.69960\n",
      "Epoch 18, CIFAR-10 Batch 5:  2017-03-30 11:10:23.708201 Cost: 0.00047\tAccuracy: 0.70000\n",
      "Epoch 19, CIFAR-10 Batch 1:  2017-03-30 11:10:27.635837 Cost: 0.00153\tAccuracy: 0.70080\n",
      "Epoch 19, CIFAR-10 Batch 2:  2017-03-30 11:10:31.577106 Cost: 0.00042\tAccuracy: 0.70600\n",
      "Epoch 19, CIFAR-10 Batch 3:  2017-03-30 11:10:35.499573 Cost: 0.00054\tAccuracy: 0.70720\n",
      "Epoch 19, CIFAR-10 Batch 4:  2017-03-30 11:10:39.423114 Cost: 0.00081\tAccuracy: 0.71420\n",
      "Epoch 19, CIFAR-10 Batch 5:  2017-03-30 11:10:43.352452 Cost: 0.00063\tAccuracy: 0.70900\n",
      "Epoch 20, CIFAR-10 Batch 1:  2017-03-30 11:10:47.292035 Cost: 0.00141\tAccuracy: 0.69840\n",
      "Epoch 20, CIFAR-10 Batch 2:  2017-03-30 11:10:51.204883 Cost: 0.00018\tAccuracy: 0.71380\n",
      "Epoch 20, CIFAR-10 Batch 3:  2017-03-30 11:10:55.126402 Cost: 0.00017\tAccuracy: 0.69960\n",
      "Epoch 20, CIFAR-10 Batch 4:  2017-03-30 11:10:59.049627 Cost: 0.00047\tAccuracy: 0.70360\n",
      "Epoch 20, CIFAR-10 Batch 5:  2017-03-30 11:11:02.975447 Cost: 0.00056\tAccuracy: 0.70440\n",
      "Epoch 21, CIFAR-10 Batch 1:  2017-03-30 11:11:06.883450 Cost: 0.00096\tAccuracy: 0.68840\n",
      "Epoch 21, CIFAR-10 Batch 2:  2017-03-30 11:11:10.828952 Cost: 0.00066\tAccuracy: 0.69220\n",
      "Epoch 21, CIFAR-10 Batch 3:  2017-03-30 11:11:14.754300 Cost: 0.00027\tAccuracy: 0.70760\n",
      "Epoch 21, CIFAR-10 Batch 4:  2017-03-30 11:11:18.674308 Cost: 0.00042\tAccuracy: 0.71460\n",
      "Epoch 21, CIFAR-10 Batch 5:  2017-03-30 11:11:22.609207 Cost: 0.00018\tAccuracy: 0.70620\n",
      "Epoch 22, CIFAR-10 Batch 1:  2017-03-30 11:11:26.543312 Cost: 0.00176\tAccuracy: 0.68940\n",
      "Epoch 22, CIFAR-10 Batch 2:  2017-03-30 11:11:30.470833 Cost: 0.00053\tAccuracy: 0.70180\n",
      "Epoch 22, CIFAR-10 Batch 3:  2017-03-30 11:11:34.374988 Cost: 0.00033\tAccuracy: 0.69820\n",
      "Epoch 22, CIFAR-10 Batch 4:  2017-03-30 11:11:38.291902 Cost: 0.00029\tAccuracy: 0.69840\n",
      "Epoch 22, CIFAR-10 Batch 5:  2017-03-30 11:11:42.201845 Cost: 0.00037\tAccuracy: 0.70260\n",
      "Epoch 23, CIFAR-10 Batch 1:  2017-03-30 11:11:46.113792 Cost: 0.00164\tAccuracy: 0.70020\n",
      "Epoch 23, CIFAR-10 Batch 2:  2017-03-30 11:11:50.015391 Cost: 0.00041\tAccuracy: 0.70440\n",
      "Epoch 23, CIFAR-10 Batch 3:  2017-03-30 11:11:53.919286 Cost: 0.00009\tAccuracy: 0.70200\n",
      "Epoch 23, CIFAR-10 Batch 4:  2017-03-30 11:11:57.816431 Cost: 0.00065\tAccuracy: 0.69400\n",
      "Epoch 23, CIFAR-10 Batch 5:  2017-03-30 11:12:01.722509 Cost: 0.00088\tAccuracy: 0.69860\n",
      "Epoch 24, CIFAR-10 Batch 1:  2017-03-30 11:12:05.634605 Cost: 0.00231\tAccuracy: 0.70160\n",
      "Epoch 24, CIFAR-10 Batch 2:  2017-03-30 11:12:09.542324 Cost: 0.00089\tAccuracy: 0.70760\n",
      "Epoch 24, CIFAR-10 Batch 3:  2017-03-30 11:12:13.450669 Cost: 0.00046\tAccuracy: 0.70240\n",
      "Epoch 24, CIFAR-10 Batch 4:  2017-03-30 11:12:17.351588 Cost: 0.00033\tAccuracy: 0.70560\n",
      "Epoch 24, CIFAR-10 Batch 5:  2017-03-30 11:12:21.251157 Cost: 0.00119\tAccuracy: 0.70980\n",
      "Epoch 25, CIFAR-10 Batch 1:  2017-03-30 11:12:25.155569 Cost: 0.00071\tAccuracy: 0.70580\n",
      "Epoch 25, CIFAR-10 Batch 2:  2017-03-30 11:12:29.060462 Cost: 0.00023\tAccuracy: 0.70140\n",
      "Epoch 25, CIFAR-10 Batch 3:  2017-03-30 11:12:32.984886 Cost: 0.00163\tAccuracy: 0.69320\n",
      "Epoch 25, CIFAR-10 Batch 4:  2017-03-30 11:12:36.891025 Cost: 0.00164\tAccuracy: 0.70600\n",
      "Epoch 25, CIFAR-10 Batch 5:  2017-03-30 11:12:40.790604 Cost: 0.00013\tAccuracy: 0.71680\n",
      "Epoch 26, CIFAR-10 Batch 1:  2017-03-30 11:12:44.691195 Cost: 0.00077\tAccuracy: 0.70320\n",
      "Epoch 26, CIFAR-10 Batch 2:  2017-03-30 11:12:48.592745 Cost: 0.00278\tAccuracy: 0.70600\n",
      "Epoch 26, CIFAR-10 Batch 3:  2017-03-30 11:12:52.496541 Cost: 0.00013\tAccuracy: 0.70000\n",
      "Epoch 26, CIFAR-10 Batch 4:  2017-03-30 11:12:56.397320 Cost: 0.00029\tAccuracy: 0.70220\n",
      "Epoch 26, CIFAR-10 Batch 5:  2017-03-30 11:13:00.291293 Cost: 0.00053\tAccuracy: 0.71000\n",
      "Epoch 27, CIFAR-10 Batch 1:  2017-03-30 11:13:04.199163 Cost: 0.00078\tAccuracy: 0.70880\n",
      "Epoch 27, CIFAR-10 Batch 2:  2017-03-30 11:13:08.104342 Cost: 0.00006\tAccuracy: 0.69360\n",
      "Epoch 27, CIFAR-10 Batch 3:  2017-03-30 11:13:12.004269 Cost: 0.00019\tAccuracy: 0.70520\n",
      "Epoch 27, CIFAR-10 Batch 4:  2017-03-30 11:13:15.908056 Cost: 0.00072\tAccuracy: 0.69220\n",
      "Epoch 27, CIFAR-10 Batch 5:  2017-03-30 11:13:19.806360 Cost: 0.00019\tAccuracy: 0.71440\n",
      "Epoch 28, CIFAR-10 Batch 1:  2017-03-30 11:13:23.707427 Cost: 0.00021\tAccuracy: 0.71340\n",
      "Epoch 28, CIFAR-10 Batch 2:  2017-03-30 11:13:27.622639 Cost: 0.00372\tAccuracy: 0.69680\n",
      "Epoch 28, CIFAR-10 Batch 3:  2017-03-30 11:13:31.541893 Cost: 0.00057\tAccuracy: 0.70440\n",
      "Epoch 28, CIFAR-10 Batch 4:  2017-03-30 11:13:35.462207 Cost: 0.00054\tAccuracy: 0.68940\n",
      "Epoch 28, CIFAR-10 Batch 5:  2017-03-30 11:13:39.356500 Cost: 0.00014\tAccuracy: 0.71540\n",
      "Epoch 29, CIFAR-10 Batch 1:  2017-03-30 11:13:43.257557 Cost: 0.00009\tAccuracy: 0.71620\n",
      "Epoch 29, CIFAR-10 Batch 2:  2017-03-30 11:13:47.156180 Cost: 0.00014\tAccuracy: 0.70540\n",
      "Epoch 29, CIFAR-10 Batch 3:  2017-03-30 11:13:51.055729 Cost: 0.00006\tAccuracy: 0.70540\n",
      "Epoch 29, CIFAR-10 Batch 4:  2017-03-30 11:13:54.950599 Cost: 0.00074\tAccuracy: 0.67720\n",
      "Epoch 29, CIFAR-10 Batch 5:  2017-03-30 11:13:58.846085 Cost: 0.00071\tAccuracy: 0.71020\n",
      "Epoch 30, CIFAR-10 Batch 1:  2017-03-30 11:14:02.752865 Cost: 0.00051\tAccuracy: 0.70800\n",
      "Epoch 30, CIFAR-10 Batch 2:  2017-03-30 11:14:06.669142 Cost: 0.00073\tAccuracy: 0.70140\n",
      "Epoch 30, CIFAR-10 Batch 3:  2017-03-30 11:14:10.590679 Cost: 0.00003\tAccuracy: 0.70780\n",
      "Epoch 30, CIFAR-10 Batch 4:  2017-03-30 11:14:14.494902 Cost: 0.00027\tAccuracy: 0.70240\n",
      "Epoch 30, CIFAR-10 Batch 5:  2017-03-30 11:14:18.414794 Cost: 0.00028\tAccuracy: 0.71100\n",
      "Epoch 31, CIFAR-10 Batch 1:  2017-03-30 11:14:22.345045 Cost: 0.00022\tAccuracy: 0.71180\n",
      "Epoch 31, CIFAR-10 Batch 2:  2017-03-30 11:14:26.263824 Cost: 0.00030\tAccuracy: 0.70180\n",
      "Epoch 31, CIFAR-10 Batch 3:  2017-03-30 11:14:30.166275 Cost: 0.00023\tAccuracy: 0.69780\n",
      "Epoch 31, CIFAR-10 Batch 4:  2017-03-30 11:14:34.065995 Cost: 0.00017\tAccuracy: 0.69760\n",
      "Epoch 31, CIFAR-10 Batch 5:  2017-03-30 11:14:37.966567 Cost: 0.00016\tAccuracy: 0.70480\n",
      "Epoch 32, CIFAR-10 Batch 1:  2017-03-30 11:14:41.879224 Cost: 0.00004\tAccuracy: 0.70500\n",
      "Epoch 32, CIFAR-10 Batch 2:  2017-03-30 11:14:45.789885 Cost: 0.00014\tAccuracy: 0.70060\n",
      "Epoch 32, CIFAR-10 Batch 3:  2017-03-30 11:14:49.692374 Cost: 0.00045\tAccuracy: 0.69800\n",
      "Epoch 32, CIFAR-10 Batch 4:  2017-03-30 11:14:53.594234 Cost: 0.00019\tAccuracy: 0.69880\n",
      "Epoch 32, CIFAR-10 Batch 5:  2017-03-30 11:14:57.491569 Cost: 0.00028\tAccuracy: 0.71200\n",
      "Epoch 33, CIFAR-10 Batch 1:  2017-03-30 11:15:01.395695 Cost: 0.00029\tAccuracy: 0.70380\n",
      "Epoch 33, CIFAR-10 Batch 2:  2017-03-30 11:15:05.306957 Cost: 0.00077\tAccuracy: 0.71000\n",
      "Epoch 33, CIFAR-10 Batch 3:  2017-03-30 11:15:09.208323 Cost: 0.00016\tAccuracy: 0.70640\n",
      "Epoch 33, CIFAR-10 Batch 4:  2017-03-30 11:15:13.132649 Cost: 0.00010\tAccuracy: 0.69960\n",
      "Epoch 33, CIFAR-10 Batch 5:  2017-03-30 11:15:17.049073 Cost: 0.00012\tAccuracy: 0.70760\n",
      "Epoch 34, CIFAR-10 Batch 1:  2017-03-30 11:15:20.952796 Cost: 0.00009\tAccuracy: 0.71180\n",
      "Epoch 34, CIFAR-10 Batch 2:  2017-03-30 11:15:24.857351 Cost: 0.00231\tAccuracy: 0.70100\n",
      "Epoch 34, CIFAR-10 Batch 3:  2017-03-30 11:15:28.761593 Cost: 0.00016\tAccuracy: 0.71180\n",
      "Epoch 34, CIFAR-10 Batch 4:  2017-03-30 11:15:32.666326 Cost: 0.00028\tAccuracy: 0.69660\n",
      "Epoch 34, CIFAR-10 Batch 5:  2017-03-30 11:15:36.577634 Cost: 0.00015\tAccuracy: 0.70780\n",
      "Epoch 35, CIFAR-10 Batch 1:  2017-03-30 11:15:40.484196 Cost: 0.00226\tAccuracy: 0.71040\n",
      "Epoch 35, CIFAR-10 Batch 2:  2017-03-30 11:15:44.393265 Cost: 0.00021\tAccuracy: 0.70300\n",
      "Epoch 35, CIFAR-10 Batch 3:  2017-03-30 11:15:48.297830 Cost: 0.00003\tAccuracy: 0.70660\n",
      "Epoch 35, CIFAR-10 Batch 4:  2017-03-30 11:15:52.220269 Cost: 0.00062\tAccuracy: 0.68740\n",
      "Epoch 35, CIFAR-10 Batch 5:  2017-03-30 11:15:56.118749 Cost: 0.00033\tAccuracy: 0.70340\n",
      "Epoch 36, CIFAR-10 Batch 1:  2017-03-30 11:16:00.020086 Cost: 0.00014\tAccuracy: 0.70840\n",
      "Epoch 36, CIFAR-10 Batch 2:  2017-03-30 11:16:03.934846 Cost: 0.00013\tAccuracy: 0.70960\n",
      "Epoch 36, CIFAR-10 Batch 3:  2017-03-30 11:16:07.838565 Cost: 0.00008\tAccuracy: 0.69860\n",
      "Epoch 36, CIFAR-10 Batch 4:  2017-03-30 11:16:11.745551 Cost: 0.00010\tAccuracy: 0.70680\n",
      "Epoch 36, CIFAR-10 Batch 5:  2017-03-30 11:16:15.640131 Cost: 0.00011\tAccuracy: 0.71060\n",
      "Epoch 37, CIFAR-10 Batch 1:  2017-03-30 11:16:19.540061 Cost: 0.00011\tAccuracy: 0.70420\n",
      "Epoch 37, CIFAR-10 Batch 2:  2017-03-30 11:16:23.448469 Cost: 0.00033\tAccuracy: 0.70760\n",
      "Epoch 37, CIFAR-10 Batch 3:  2017-03-30 11:16:27.361066 Cost: 0.00003\tAccuracy: 0.69900\n",
      "Epoch 37, CIFAR-10 Batch 4:  2017-03-30 11:16:31.264151 Cost: 0.00006\tAccuracy: 0.70400\n",
      "Epoch 37, CIFAR-10 Batch 5:  2017-03-30 11:16:35.169826 Cost: 0.00009\tAccuracy: 0.70620\n",
      "Epoch 38, CIFAR-10 Batch 1:  2017-03-30 11:16:39.079216 Cost: 0.00006\tAccuracy: 0.71480\n",
      "Epoch 38, CIFAR-10 Batch 2:  2017-03-30 11:16:42.993760 Cost: 0.00010\tAccuracy: 0.71000\n",
      "Epoch 38, CIFAR-10 Batch 3:  2017-03-30 11:16:46.916903 Cost: 0.00017\tAccuracy: 0.70240\n",
      "Epoch 38, CIFAR-10 Batch 4:  2017-03-30 11:16:50.821737 Cost: 0.00028\tAccuracy: 0.70140\n",
      "Epoch 38, CIFAR-10 Batch 5:  2017-03-30 11:16:54.738336 Cost: 0.00057\tAccuracy: 0.71340\n",
      "Epoch 39, CIFAR-10 Batch 1:  2017-03-30 11:16:58.649408 Cost: 0.00063\tAccuracy: 0.71140\n",
      "Epoch 39, CIFAR-10 Batch 2:  2017-03-30 11:17:02.570421 Cost: 0.00012\tAccuracy: 0.70700\n",
      "Epoch 39, CIFAR-10 Batch 3:  2017-03-30 11:17:06.493305 Cost: 0.00007\tAccuracy: 0.71060\n",
      "Epoch 39, CIFAR-10 Batch 4:  2017-03-30 11:17:10.410514 Cost: 0.00023\tAccuracy: 0.70720\n",
      "Epoch 39, CIFAR-10 Batch 5:  2017-03-30 11:17:14.323945 Cost: 0.00069\tAccuracy: 0.70580\n",
      "Epoch 40, CIFAR-10 Batch 1:  2017-03-30 11:17:18.240563 Cost: 0.00037\tAccuracy: 0.70480\n",
      "Epoch 40, CIFAR-10 Batch 2:  2017-03-30 11:17:22.184332 Cost: 0.00005\tAccuracy: 0.71440\n",
      "Epoch 40, CIFAR-10 Batch 3:  2017-03-30 11:17:26.118813 Cost: 0.00003\tAccuracy: 0.71420\n",
      "Epoch 40, CIFAR-10 Batch 4:  2017-03-30 11:17:30.060092 Cost: 0.00021\tAccuracy: 0.70520\n",
      "Epoch 40, CIFAR-10 Batch 5:  2017-03-30 11:17:33.996874 Cost: 0.00037\tAccuracy: 0.71200\n",
      "Epoch 41, CIFAR-10 Batch 1:  2017-03-30 11:17:37.931940 Cost: 0.00002\tAccuracy: 0.71020\n",
      "Epoch 41, CIFAR-10 Batch 2:  2017-03-30 11:17:41.867955 Cost: 0.00004\tAccuracy: 0.71460\n",
      "Epoch 41, CIFAR-10 Batch 3:  2017-03-30 11:17:45.813293 Cost: 0.00004\tAccuracy: 0.70840\n",
      "Epoch 41, CIFAR-10 Batch 4:  2017-03-30 11:17:49.737782 Cost: 0.00002\tAccuracy: 0.70720\n",
      "Epoch 41, CIFAR-10 Batch 5:  2017-03-30 11:17:53.645755 Cost: 0.00005\tAccuracy: 0.71060\n",
      "Epoch 42, CIFAR-10 Batch 1:  2017-03-30 11:17:57.582885 Cost: 0.00045\tAccuracy: 0.70960\n",
      "Epoch 42, CIFAR-10 Batch 2:  2017-03-30 11:18:01.524244 Cost: 0.00003\tAccuracy: 0.70480\n",
      "Epoch 42, CIFAR-10 Batch 3:  2017-03-30 11:18:05.462370 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 42, CIFAR-10 Batch 4:  2017-03-30 11:18:09.383047 Cost: 0.00002\tAccuracy: 0.71560\n",
      "Epoch 42, CIFAR-10 Batch 5:  2017-03-30 11:18:13.298719 Cost: 0.00003\tAccuracy: 0.72040\n",
      "Epoch 43, CIFAR-10 Batch 1:  2017-03-30 11:18:17.228174 Cost: 0.00010\tAccuracy: 0.70840\n",
      "Epoch 43, CIFAR-10 Batch 2:  2017-03-30 11:18:21.162359 Cost: 0.00026\tAccuracy: 0.70620\n",
      "Epoch 43, CIFAR-10 Batch 3:  2017-03-30 11:18:25.083707 Cost: 0.02547\tAccuracy: 0.70120\n",
      "Epoch 43, CIFAR-10 Batch 4:  2017-03-30 11:18:29.010696 Cost: 0.00022\tAccuracy: 0.70700\n",
      "Epoch 43, CIFAR-10 Batch 5:  2017-03-30 11:18:32.928048 Cost: 0.00006\tAccuracy: 0.71300\n",
      "Epoch 44, CIFAR-10 Batch 1:  2017-03-30 11:18:36.857935 Cost: 0.00009\tAccuracy: 0.71480\n",
      "Epoch 44, CIFAR-10 Batch 2:  2017-03-30 11:18:40.784425 Cost: 0.00002\tAccuracy: 0.71880\n",
      "Epoch 44, CIFAR-10 Batch 3:  2017-03-30 11:18:44.716257 Cost: 0.00011\tAccuracy: 0.71160\n",
      "Epoch 44, CIFAR-10 Batch 4:  2017-03-30 11:18:48.650690 Cost: 0.00008\tAccuracy: 0.71160\n",
      "Epoch 44, CIFAR-10 Batch 5:  2017-03-30 11:18:52.563732 Cost: 0.00007\tAccuracy: 0.71000\n",
      "Epoch 45, CIFAR-10 Batch 1:  2017-03-30 11:18:56.487607 Cost: 0.00014\tAccuracy: 0.71440\n",
      "Epoch 45, CIFAR-10 Batch 2:  2017-03-30 11:19:00.414696 Cost: 0.00002\tAccuracy: 0.71100\n",
      "Epoch 45, CIFAR-10 Batch 3:  2017-03-30 11:19:04.331131 Cost: 0.00022\tAccuracy: 0.70900\n",
      "Epoch 45, CIFAR-10 Batch 4:  2017-03-30 11:19:08.256554 Cost: 0.00005\tAccuracy: 0.70160\n",
      "Epoch 45, CIFAR-10 Batch 5:  2017-03-30 11:19:12.197644 Cost: 0.00009\tAccuracy: 0.71020\n",
      "Epoch 46, CIFAR-10 Batch 1:  2017-03-30 11:19:16.137467 Cost: 0.00042\tAccuracy: 0.71460\n",
      "Epoch 46, CIFAR-10 Batch 2:  2017-03-30 11:19:20.096856 Cost: 0.00048\tAccuracy: 0.70400\n",
      "Epoch 46, CIFAR-10 Batch 3:  2017-03-30 11:19:24.043003 Cost: 0.00007\tAccuracy: 0.70660\n",
      "Epoch 46, CIFAR-10 Batch 4:  2017-03-30 11:19:27.989980 Cost: 0.00001\tAccuracy: 0.70940\n",
      "Epoch 46, CIFAR-10 Batch 5:  2017-03-30 11:19:31.935898 Cost: 0.00039\tAccuracy: 0.70620\n",
      "Epoch 47, CIFAR-10 Batch 1:  2017-03-30 11:19:35.892294 Cost: 0.00008\tAccuracy: 0.71020\n",
      "Epoch 47, CIFAR-10 Batch 2:  2017-03-30 11:19:39.849056 Cost: 0.00006\tAccuracy: 0.70160\n",
      "Epoch 47, CIFAR-10 Batch 3:  2017-03-30 11:19:43.801214 Cost: 0.00012\tAccuracy: 0.70680\n",
      "Epoch 47, CIFAR-10 Batch 4:  2017-03-30 11:19:47.749180 Cost: 0.00005\tAccuracy: 0.71160\n",
      "Epoch 47, CIFAR-10 Batch 5:  2017-03-30 11:19:51.692318 Cost: 0.00125\tAccuracy: 0.70940\n",
      "Epoch 48, CIFAR-10 Batch 1:  2017-03-30 11:19:55.634957 Cost: 0.00064\tAccuracy: 0.71280\n",
      "Epoch 48, CIFAR-10 Batch 2:  2017-03-30 11:19:59.586645 Cost: 0.00003\tAccuracy: 0.71400\n",
      "Epoch 48, CIFAR-10 Batch 3:  2017-03-30 11:20:03.547350 Cost: 0.00005\tAccuracy: 0.70120\n",
      "Epoch 48, CIFAR-10 Batch 4:  2017-03-30 11:20:07.482866 Cost: 0.00002\tAccuracy: 0.70960\n",
      "Epoch 48, CIFAR-10 Batch 5:  2017-03-30 11:20:11.418823 Cost: 0.00007\tAccuracy: 0.70600\n",
      "Epoch 49, CIFAR-10 Batch 1:  2017-03-30 11:20:15.358900 Cost: 0.00011\tAccuracy: 0.71420\n",
      "Epoch 49, CIFAR-10 Batch 2:  2017-03-30 11:20:19.300141 Cost: 0.00001\tAccuracy: 0.71820\n",
      "Epoch 49, CIFAR-10 Batch 3:  2017-03-30 11:20:23.256605 Cost: 0.00005\tAccuracy: 0.70940\n",
      "Epoch 49, CIFAR-10 Batch 4:  2017-03-30 11:20:27.194124 Cost: 0.00005\tAccuracy: 0.70740\n",
      "Epoch 49, CIFAR-10 Batch 5:  2017-03-30 11:20:31.138123 Cost: 0.00020\tAccuracy: 0.70020\n",
      "Epoch 50, CIFAR-10 Batch 1:  2017-03-30 11:20:35.093679 Cost: 0.00010\tAccuracy: 0.70560\n",
      "Epoch 50, CIFAR-10 Batch 2:  2017-03-30 11:20:39.050141 Cost: 0.00012\tAccuracy: 0.70740\n",
      "Epoch 50, CIFAR-10 Batch 3:  2017-03-30 11:20:42.992013 Cost: 0.00002\tAccuracy: 0.70760\n",
      "Epoch 50, CIFAR-10 Batch 4:  2017-03-30 11:20:46.941039 Cost: 0.00004\tAccuracy: 0.69940\n",
      "Epoch 50, CIFAR-10 Batch 5:  2017-03-30 11:20:50.887129 Cost: 0.00028\tAccuracy: 0.71040\n",
      "Epoch 51, CIFAR-10 Batch 1:  2017-03-30 11:20:54.841763 Cost: 0.00012\tAccuracy: 0.71520\n",
      "Epoch 51, CIFAR-10 Batch 2:  2017-03-30 11:20:58.782624 Cost: 0.00001\tAccuracy: 0.70640\n",
      "Epoch 51, CIFAR-10 Batch 3:  2017-03-30 11:21:02.723577 Cost: 0.00001\tAccuracy: 0.71040\n",
      "Epoch 51, CIFAR-10 Batch 4:  2017-03-30 11:21:06.647570 Cost: 0.00010\tAccuracy: 0.69820\n",
      "Epoch 51, CIFAR-10 Batch 5:  2017-03-30 11:21:10.578260 Cost: 0.00003\tAccuracy: 0.71120\n",
      "Epoch 52, CIFAR-10 Batch 1:  2017-03-30 11:21:14.514452 Cost: 0.00036\tAccuracy: 0.70780\n",
      "Epoch 52, CIFAR-10 Batch 2:  2017-03-30 11:21:18.466597 Cost: 0.00004\tAccuracy: 0.70740\n",
      "Epoch 52, CIFAR-10 Batch 3:  2017-03-30 11:21:22.419599 Cost: 0.00003\tAccuracy: 0.70780\n",
      "Epoch 52, CIFAR-10 Batch 4:  2017-03-30 11:21:26.352639 Cost: 0.00004\tAccuracy: 0.71160\n",
      "Epoch 52, CIFAR-10 Batch 5:  2017-03-30 11:21:30.266311 Cost: 0.00001\tAccuracy: 0.71480\n",
      "Epoch 53, CIFAR-10 Batch 1:  2017-03-30 11:21:34.203056 Cost: 0.00011\tAccuracy: 0.71140\n",
      "Epoch 53, CIFAR-10 Batch 2:  2017-03-30 11:21:38.150008 Cost: 0.00009\tAccuracy: 0.70020\n",
      "Epoch 53, CIFAR-10 Batch 3:  2017-03-30 11:21:42.082303 Cost: 0.00005\tAccuracy: 0.70780\n",
      "Epoch 53, CIFAR-10 Batch 4:  2017-03-30 11:21:46.015878 Cost: 0.00005\tAccuracy: 0.71360\n",
      "Epoch 53, CIFAR-10 Batch 5:  2017-03-30 11:21:49.954361 Cost: 0.00016\tAccuracy: 0.70720\n",
      "Epoch 54, CIFAR-10 Batch 1:  2017-03-30 11:21:53.884490 Cost: 0.00020\tAccuracy: 0.71100\n",
      "Epoch 54, CIFAR-10 Batch 2:  2017-03-30 11:21:57.826164 Cost: 0.00001\tAccuracy: 0.70500\n",
      "Epoch 54, CIFAR-10 Batch 3:  2017-03-30 11:22:01.771817 Cost: 0.00000\tAccuracy: 0.70720\n",
      "Epoch 54, CIFAR-10 Batch 4:  2017-03-30 11:22:05.706544 Cost: 0.00001\tAccuracy: 0.71420\n",
      "Epoch 54, CIFAR-10 Batch 5:  2017-03-30 11:22:09.635960 Cost: 0.00004\tAccuracy: 0.70900\n",
      "Epoch 55, CIFAR-10 Batch 1:  2017-03-30 11:22:13.578874 Cost: 0.00004\tAccuracy: 0.71320\n",
      "Epoch 55, CIFAR-10 Batch 2:  2017-03-30 11:22:17.523460 Cost: 0.00003\tAccuracy: 0.71140\n",
      "Epoch 55, CIFAR-10 Batch 3:  2017-03-30 11:22:21.470870 Cost: 0.00004\tAccuracy: 0.70800\n",
      "Epoch 55, CIFAR-10 Batch 4:  2017-03-30 11:22:25.409878 Cost: 0.00002\tAccuracy: 0.71120\n",
      "Epoch 55, CIFAR-10 Batch 5:  2017-03-30 11:22:29.355231 Cost: 0.00036\tAccuracy: 0.70780\n",
      "Epoch 56, CIFAR-10 Batch 1:  2017-03-30 11:22:33.297531 Cost: 0.00002\tAccuracy: 0.72100\n",
      "Epoch 56, CIFAR-10 Batch 2:  2017-03-30 11:22:37.227114 Cost: 0.00002\tAccuracy: 0.70500\n",
      "Epoch 56, CIFAR-10 Batch 3:  2017-03-30 11:22:41.170894 Cost: 0.00000\tAccuracy: 0.70800\n",
      "Epoch 56, CIFAR-10 Batch 4:  2017-03-30 11:22:45.121108 Cost: 0.00001\tAccuracy: 0.71320\n",
      "Epoch 56, CIFAR-10 Batch 5:  2017-03-30 11:22:49.056937 Cost: 0.00003\tAccuracy: 0.70820\n",
      "Epoch 57, CIFAR-10 Batch 1:  2017-03-30 11:22:52.994124 Cost: 0.00005\tAccuracy: 0.71420\n",
      "Epoch 57, CIFAR-10 Batch 2:  2017-03-30 11:22:56.918327 Cost: 0.00002\tAccuracy: 0.70460\n",
      "Epoch 57, CIFAR-10 Batch 3:  2017-03-30 11:23:00.834090 Cost: 0.00000\tAccuracy: 0.71600\n",
      "Epoch 57, CIFAR-10 Batch 4:  2017-03-30 11:23:04.759810 Cost: 0.00002\tAccuracy: 0.71180\n",
      "Epoch 57, CIFAR-10 Batch 5:  2017-03-30 11:23:08.673187 Cost: 0.00002\tAccuracy: 0.70820\n",
      "Epoch 58, CIFAR-10 Batch 1:  2017-03-30 11:23:12.600593 Cost: 0.00009\tAccuracy: 0.70940\n",
      "Epoch 58, CIFAR-10 Batch 2:  2017-03-30 11:23:16.502851 Cost: 0.00002\tAccuracy: 0.71020\n",
      "Epoch 58, CIFAR-10 Batch 3:  2017-03-30 11:23:20.409844 Cost: 0.00001\tAccuracy: 0.71420\n",
      "Epoch 58, CIFAR-10 Batch 4:  2017-03-30 11:23:24.311147 Cost: 0.00013\tAccuracy: 0.71180\n",
      "Epoch 58, CIFAR-10 Batch 5:  2017-03-30 11:23:28.211679 Cost: 0.00000\tAccuracy: 0.71320\n",
      "Epoch 59, CIFAR-10 Batch 1:  2017-03-30 11:23:32.126601 Cost: 0.00073\tAccuracy: 0.70400\n",
      "Epoch 59, CIFAR-10 Batch 2:  2017-03-30 11:23:36.062730 Cost: 0.00004\tAccuracy: 0.71100\n",
      "Epoch 59, CIFAR-10 Batch 3:  2017-03-30 11:23:39.992668 Cost: 0.00002\tAccuracy: 0.71140\n",
      "Epoch 59, CIFAR-10 Batch 4:  2017-03-30 11:23:43.920785 Cost: 0.00002\tAccuracy: 0.71680\n",
      "Epoch 59, CIFAR-10 Batch 5:  2017-03-30 11:23:47.850749 Cost: 0.00005\tAccuracy: 0.70520\n",
      "Epoch 60, CIFAR-10 Batch 1:  2017-03-30 11:23:51.790386 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 60, CIFAR-10 Batch 2:  2017-03-30 11:23:55.723645 Cost: 0.00000\tAccuracy: 0.71820\n",
      "Epoch 60, CIFAR-10 Batch 3:  2017-03-30 11:23:59.643046 Cost: 0.00001\tAccuracy: 0.71260\n",
      "Epoch 60, CIFAR-10 Batch 4:  2017-03-30 11:24:03.561151 Cost: 0.00008\tAccuracy: 0.71120\n",
      "Epoch 60, CIFAR-10 Batch 5:  2017-03-30 11:24:07.493585 Cost: 0.00001\tAccuracy: 0.71520\n",
      "Epoch 61, CIFAR-10 Batch 1:  2017-03-30 11:24:11.415339 Cost: 0.00002\tAccuracy: 0.71160\n",
      "Epoch 61, CIFAR-10 Batch 2:  2017-03-30 11:24:15.344354 Cost: 0.00002\tAccuracy: 0.70980\n",
      "Epoch 61, CIFAR-10 Batch 3:  2017-03-30 11:24:19.272644 Cost: 0.00000\tAccuracy: 0.70180\n",
      "Epoch 61, CIFAR-10 Batch 4:  2017-03-30 11:24:23.191505 Cost: 0.00005\tAccuracy: 0.71060\n",
      "Epoch 61, CIFAR-10 Batch 5:  2017-03-30 11:24:27.115033 Cost: 0.00004\tAccuracy: 0.70840\n",
      "Epoch 62, CIFAR-10 Batch 1:  2017-03-30 11:24:31.051710 Cost: 0.00000\tAccuracy: 0.70680\n",
      "Epoch 62, CIFAR-10 Batch 2:  2017-03-30 11:24:34.985356 Cost: 0.00025\tAccuracy: 0.70420\n",
      "Epoch 62, CIFAR-10 Batch 3:  2017-03-30 11:24:38.919774 Cost: 0.00001\tAccuracy: 0.70140\n",
      "Epoch 62, CIFAR-10 Batch 4:  2017-03-30 11:24:42.840159 Cost: 0.00011\tAccuracy: 0.70800\n",
      "Epoch 62, CIFAR-10 Batch 5:  2017-03-30 11:24:46.776957 Cost: 0.00031\tAccuracy: 0.70660\n",
      "Epoch 63, CIFAR-10 Batch 1:  2017-03-30 11:24:50.734247 Cost: 0.00000\tAccuracy: 0.70940\n",
      "Epoch 63, CIFAR-10 Batch 2:  2017-03-30 11:24:54.678910 Cost: 0.00011\tAccuracy: 0.70220\n",
      "Epoch 63, CIFAR-10 Batch 3:  2017-03-30 11:24:58.599557 Cost: 0.00001\tAccuracy: 0.70660\n",
      "Epoch 63, CIFAR-10 Batch 4:  2017-03-30 11:25:02.521190 Cost: 0.00001\tAccuracy: 0.70240\n",
      "Epoch 63, CIFAR-10 Batch 5:  2017-03-30 11:25:06.444691 Cost: 0.00004\tAccuracy: 0.71100\n",
      "Epoch 64, CIFAR-10 Batch 1:  2017-03-30 11:25:10.368820 Cost: 0.00018\tAccuracy: 0.70340\n",
      "Epoch 64, CIFAR-10 Batch 2:  2017-03-30 11:25:14.292233 Cost: 0.00004\tAccuracy: 0.70960\n",
      "Epoch 64, CIFAR-10 Batch 3:  2017-03-30 11:25:18.229434 Cost: 0.00000\tAccuracy: 0.71400\n",
      "Epoch 64, CIFAR-10 Batch 4:  2017-03-30 11:25:22.158421 Cost: 0.00002\tAccuracy: 0.70620\n",
      "Epoch 64, CIFAR-10 Batch 5:  2017-03-30 11:25:26.083749 Cost: 0.00003\tAccuracy: 0.71260\n",
      "Epoch 65, CIFAR-10 Batch 1:  2017-03-30 11:25:30.031402 Cost: 0.00013\tAccuracy: 0.70960\n",
      "Epoch 65, CIFAR-10 Batch 2:  2017-03-30 11:25:33.982441 Cost: 0.00005\tAccuracy: 0.70880\n",
      "Epoch 65, CIFAR-10 Batch 3:  2017-03-30 11:25:37.925753 Cost: 0.00001\tAccuracy: 0.70460\n",
      "Epoch 65, CIFAR-10 Batch 4:  2017-03-30 11:25:41.847588 Cost: 0.00001\tAccuracy: 0.71660\n",
      "Epoch 65, CIFAR-10 Batch 5:  2017-03-30 11:25:45.769551 Cost: 0.00001\tAccuracy: 0.70940\n",
      "Epoch 66, CIFAR-10 Batch 1:  2017-03-30 11:25:49.700406 Cost: 0.00003\tAccuracy: 0.70980\n",
      "Epoch 66, CIFAR-10 Batch 2:  2017-03-30 11:25:53.610258 Cost: 0.00006\tAccuracy: 0.71080\n",
      "Epoch 66, CIFAR-10 Batch 3:  2017-03-30 11:25:57.520726 Cost: 0.00003\tAccuracy: 0.70200\n",
      "Epoch 66, CIFAR-10 Batch 4:  2017-03-30 11:26:01.444836 Cost: 0.00003\tAccuracy: 0.71480\n",
      "Epoch 66, CIFAR-10 Batch 5:  2017-03-30 11:26:05.370908 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 67, CIFAR-10 Batch 1:  2017-03-30 11:26:09.319387 Cost: 0.00002\tAccuracy: 0.71400\n",
      "Epoch 67, CIFAR-10 Batch 2:  2017-03-30 11:26:13.265254 Cost: 0.00002\tAccuracy: 0.70640\n",
      "Epoch 67, CIFAR-10 Batch 3:  2017-03-30 11:26:17.221059 Cost: 0.00001\tAccuracy: 0.70560\n",
      "Epoch 67, CIFAR-10 Batch 4:  2017-03-30 11:26:21.163604 Cost: 0.00010\tAccuracy: 0.70600\n",
      "Epoch 67, CIFAR-10 Batch 5:  2017-03-30 11:26:25.094605 Cost: 0.00003\tAccuracy: 0.71420\n",
      "Epoch 68, CIFAR-10 Batch 1:  2017-03-30 11:26:29.024861 Cost: 0.00001\tAccuracy: 0.71140\n",
      "Epoch 68, CIFAR-10 Batch 2:  2017-03-30 11:26:32.949340 Cost: 0.00007\tAccuracy: 0.70480\n",
      "Epoch 68, CIFAR-10 Batch 3:  2017-03-30 11:26:36.879364 Cost: 0.00003\tAccuracy: 0.70600\n",
      "Epoch 68, CIFAR-10 Batch 4:  2017-03-30 11:26:40.796346 Cost: 0.00011\tAccuracy: 0.71040\n",
      "Epoch 68, CIFAR-10 Batch 5:  2017-03-30 11:26:44.700840 Cost: 0.00005\tAccuracy: 0.71060\n",
      "Epoch 69, CIFAR-10 Batch 1:  2017-03-30 11:26:48.634766 Cost: 0.00007\tAccuracy: 0.71960\n",
      "Epoch 69, CIFAR-10 Batch 2:  2017-03-30 11:26:52.554688 Cost: 0.00001\tAccuracy: 0.71820\n",
      "Epoch 69, CIFAR-10 Batch 3:  2017-03-30 11:26:56.460370 Cost: 0.00002\tAccuracy: 0.70700\n",
      "Epoch 69, CIFAR-10 Batch 4:  2017-03-30 11:27:00.398350 Cost: 0.00006\tAccuracy: 0.71160\n",
      "Epoch 69, CIFAR-10 Batch 5:  2017-03-30 11:27:04.343984 Cost: 0.00031\tAccuracy: 0.71080\n",
      "Epoch 70, CIFAR-10 Batch 1:  2017-03-30 11:27:08.278418 Cost: 0.00021\tAccuracy: 0.71860\n",
      "Epoch 70, CIFAR-10 Batch 2:  2017-03-30 11:27:12.209512 Cost: 0.00047\tAccuracy: 0.70600\n",
      "Epoch 70, CIFAR-10 Batch 3:  2017-03-30 11:27:16.135529 Cost: 0.00001\tAccuracy: 0.70800\n",
      "Epoch 70, CIFAR-10 Batch 4:  2017-03-30 11:27:20.066135 Cost: 0.00000\tAccuracy: 0.71020\n",
      "Epoch 70, CIFAR-10 Batch 5:  2017-03-30 11:27:23.994147 Cost: 0.00003\tAccuracy: 0.70980\n",
      "Epoch 71, CIFAR-10 Batch 1:  2017-03-30 11:27:27.922377 Cost: 0.00005\tAccuracy: 0.71560\n",
      "Epoch 71, CIFAR-10 Batch 2:  2017-03-30 11:27:31.855758 Cost: 0.00001\tAccuracy: 0.70920\n",
      "Epoch 71, CIFAR-10 Batch 3:  2017-03-30 11:27:35.786616 Cost: 0.00001\tAccuracy: 0.71240\n",
      "Epoch 71, CIFAR-10 Batch 4:  2017-03-30 11:27:39.716176 Cost: 0.00002\tAccuracy: 0.70900\n",
      "Epoch 71, CIFAR-10 Batch 5:  2017-03-30 11:27:43.658815 Cost: 0.00006\tAccuracy: 0.71080\n",
      "Epoch 72, CIFAR-10 Batch 1:  2017-03-30 11:27:47.606912 Cost: 0.00002\tAccuracy: 0.71160\n",
      "Epoch 72, CIFAR-10 Batch 2:  2017-03-30 11:27:51.556558 Cost: 0.00012\tAccuracy: 0.70740\n",
      "Epoch 72, CIFAR-10 Batch 3:  2017-03-30 11:27:55.502842 Cost: 0.00000\tAccuracy: 0.70540\n",
      "Epoch 72, CIFAR-10 Batch 4:  2017-03-30 11:27:59.415493 Cost: 0.00002\tAccuracy: 0.70760\n",
      "Epoch 72, CIFAR-10 Batch 5:  2017-03-30 11:28:03.332061 Cost: 0.00005\tAccuracy: 0.70300\n",
      "Epoch 73, CIFAR-10 Batch 1:  2017-03-30 11:28:07.244113 Cost: 0.00007\tAccuracy: 0.71500\n",
      "Epoch 73, CIFAR-10 Batch 2:  2017-03-30 11:28:11.150155 Cost: 0.00002\tAccuracy: 0.71080\n",
      "Epoch 73, CIFAR-10 Batch 3:  2017-03-30 11:28:15.099615 Cost: 0.00000\tAccuracy: 0.71300\n",
      "Epoch 73, CIFAR-10 Batch 4:  2017-03-30 11:28:19.043978 Cost: 0.00002\tAccuracy: 0.72080\n",
      "Epoch 73, CIFAR-10 Batch 5:  2017-03-30 11:28:22.971128 Cost: 0.00003\tAccuracy: 0.70620\n",
      "Epoch 74, CIFAR-10 Batch 1:  2017-03-30 11:28:26.924657 Cost: 0.00002\tAccuracy: 0.71060\n",
      "Epoch 74, CIFAR-10 Batch 2:  2017-03-30 11:28:30.869717 Cost: 0.00075\tAccuracy: 0.71220\n",
      "Epoch 74, CIFAR-10 Batch 3:  2017-03-30 11:28:34.801827 Cost: 0.00001\tAccuracy: 0.71340\n",
      "Epoch 74, CIFAR-10 Batch 4:  2017-03-30 11:28:38.737854 Cost: 0.00001\tAccuracy: 0.70660\n",
      "Epoch 74, CIFAR-10 Batch 5:  2017-03-30 11:28:42.675728 Cost: 0.00002\tAccuracy: 0.71320\n",
      "Epoch 75, CIFAR-10 Batch 1:  2017-03-30 11:28:46.610810 Cost: 0.00009\tAccuracy: 0.71240\n",
      "Epoch 75, CIFAR-10 Batch 2:  2017-03-30 11:28:50.533976 Cost: 0.00005\tAccuracy: 0.71340\n",
      "Epoch 75, CIFAR-10 Batch 3:  2017-03-30 11:28:54.475953 Cost: 0.00010\tAccuracy: 0.70840\n",
      "Epoch 75, CIFAR-10 Batch 4:  2017-03-30 11:28:58.395350 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 75, CIFAR-10 Batch 5:  2017-03-30 11:29:02.331758 Cost: 0.00008\tAccuracy: 0.71060\n",
      "Epoch 76, CIFAR-10 Batch 1:  2017-03-30 11:29:06.262421 Cost: 0.00001\tAccuracy: 0.71240\n",
      "Epoch 76, CIFAR-10 Batch 2:  2017-03-30 11:29:10.187598 Cost: 0.00041\tAccuracy: 0.70740\n",
      "Epoch 76, CIFAR-10 Batch 3:  2017-03-30 11:29:14.120195 Cost: 0.00001\tAccuracy: 0.71260\n",
      "Epoch 76, CIFAR-10 Batch 4:  2017-03-30 11:29:18.046161 Cost: 0.00002\tAccuracy: 0.71720\n",
      "Epoch 76, CIFAR-10 Batch 5:  2017-03-30 11:29:21.947659 Cost: 0.00008\tAccuracy: 0.71060\n",
      "Epoch 77, CIFAR-10 Batch 1:  2017-03-30 11:29:25.858604 Cost: 0.00001\tAccuracy: 0.71620\n",
      "Epoch 77, CIFAR-10 Batch 2:  2017-03-30 11:29:29.759123 Cost: 0.00004\tAccuracy: 0.71760\n",
      "Epoch 77, CIFAR-10 Batch 3:  2017-03-30 11:29:33.669339 Cost: 0.00001\tAccuracy: 0.70960\n",
      "Epoch 77, CIFAR-10 Batch 4:  2017-03-30 11:29:37.573728 Cost: 0.00001\tAccuracy: 0.71820\n",
      "Epoch 77, CIFAR-10 Batch 5:  2017-03-30 11:29:41.481111 Cost: 0.00102\tAccuracy: 0.71480\n",
      "Epoch 78, CIFAR-10 Batch 1:  2017-03-30 11:29:45.385636 Cost: 0.00025\tAccuracy: 0.70780\n",
      "Epoch 78, CIFAR-10 Batch 2:  2017-03-30 11:29:49.290026 Cost: 0.00007\tAccuracy: 0.71100\n",
      "Epoch 78, CIFAR-10 Batch 3:  2017-03-30 11:29:53.193600 Cost: 0.00000\tAccuracy: 0.71580\n",
      "Epoch 78, CIFAR-10 Batch 4:  2017-03-30 11:29:57.116158 Cost: 0.00001\tAccuracy: 0.71020\n",
      "Epoch 78, CIFAR-10 Batch 5:  2017-03-30 11:30:01.049884 Cost: 0.00005\tAccuracy: 0.70980\n",
      "Epoch 79, CIFAR-10 Batch 1:  2017-03-30 11:30:04.989803 Cost: 0.00001\tAccuracy: 0.71620\n",
      "Epoch 79, CIFAR-10 Batch 2:  2017-03-30 11:30:08.929758 Cost: 0.00001\tAccuracy: 0.71220\n",
      "Epoch 79, CIFAR-10 Batch 3:  2017-03-30 11:30:12.836570 Cost: 0.00001\tAccuracy: 0.70520\n",
      "Epoch 79, CIFAR-10 Batch 4:  2017-03-30 11:30:16.732302 Cost: 0.00001\tAccuracy: 0.70840\n",
      "Epoch 79, CIFAR-10 Batch 5:  2017-03-30 11:30:20.634752 Cost: 0.00006\tAccuracy: 0.71000\n",
      "Epoch 80, CIFAR-10 Batch 1:  2017-03-30 11:30:24.536401 Cost: 0.00002\tAccuracy: 0.70700\n",
      "Epoch 80, CIFAR-10 Batch 2:  2017-03-30 11:30:28.439239 Cost: 0.00010\tAccuracy: 0.70960\n",
      "Epoch 80, CIFAR-10 Batch 3:  2017-03-30 11:30:32.334975 Cost: 0.00001\tAccuracy: 0.71420\n",
      "Epoch 80, CIFAR-10 Batch 4:  2017-03-30 11:30:36.239068 Cost: 0.00000\tAccuracy: 0.71620\n",
      "Epoch 80, CIFAR-10 Batch 5:  2017-03-30 11:30:40.144215 Cost: 0.00002\tAccuracy: 0.70900\n",
      "Epoch 81, CIFAR-10 Batch 1:  2017-03-30 11:30:44.048018 Cost: 0.00006\tAccuracy: 0.70940\n",
      "Epoch 81, CIFAR-10 Batch 2:  2017-03-30 11:30:47.959880 Cost: 0.00012\tAccuracy: 0.70960\n",
      "Epoch 81, CIFAR-10 Batch 3:  2017-03-30 11:30:51.863836 Cost: 0.00003\tAccuracy: 0.70780\n",
      "Epoch 81, CIFAR-10 Batch 4:  2017-03-30 11:30:55.762557 Cost: 0.00001\tAccuracy: 0.72140\n",
      "Epoch 81, CIFAR-10 Batch 5:  2017-03-30 11:30:59.660997 Cost: 0.00003\tAccuracy: 0.71500\n",
      "Epoch 82, CIFAR-10 Batch 1:  2017-03-30 11:31:03.573909 Cost: 0.00001\tAccuracy: 0.71860\n",
      "Epoch 82, CIFAR-10 Batch 2:  2017-03-30 11:31:07.483035 Cost: 0.00001\tAccuracy: 0.71800\n",
      "Epoch 82, CIFAR-10 Batch 3:  2017-03-30 11:31:11.389772 Cost: 0.00015\tAccuracy: 0.71520\n",
      "Epoch 82, CIFAR-10 Batch 4:  2017-03-30 11:31:15.290292 Cost: 0.00000\tAccuracy: 0.71120\n",
      "Epoch 82, CIFAR-10 Batch 5:  2017-03-30 11:31:19.187463 Cost: 0.00001\tAccuracy: 0.72120\n",
      "Epoch 83, CIFAR-10 Batch 1:  2017-03-30 11:31:23.089094 Cost: 0.00002\tAccuracy: 0.71340\n",
      "Epoch 83, CIFAR-10 Batch 2:  2017-03-30 11:31:26.991580 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 83, CIFAR-10 Batch 3:  2017-03-30 11:31:30.894340 Cost: 0.00002\tAccuracy: 0.71320\n",
      "Epoch 83, CIFAR-10 Batch 4:  2017-03-30 11:31:34.791185 Cost: 0.00002\tAccuracy: 0.71760\n",
      "Epoch 83, CIFAR-10 Batch 5:  2017-03-30 11:31:38.690503 Cost: 0.00004\tAccuracy: 0.71260\n",
      "Epoch 84, CIFAR-10 Batch 1:  2017-03-30 11:31:42.602694 Cost: 0.00001\tAccuracy: 0.70180\n",
      "Epoch 84, CIFAR-10 Batch 2:  2017-03-30 11:31:46.538947 Cost: 0.00010\tAccuracy: 0.70480\n",
      "Epoch 84, CIFAR-10 Batch 3:  2017-03-30 11:31:50.447098 Cost: 0.00013\tAccuracy: 0.71280\n",
      "Epoch 84, CIFAR-10 Batch 4:  2017-03-30 11:31:54.346074 Cost: 0.00001\tAccuracy: 0.71580\n",
      "Epoch 84, CIFAR-10 Batch 5:  2017-03-30 11:31:58.244330 Cost: 0.00000\tAccuracy: 0.72180\n",
      "Epoch 85, CIFAR-10 Batch 1:  2017-03-30 11:32:02.153656 Cost: 0.00001\tAccuracy: 0.72320\n",
      "Epoch 85, CIFAR-10 Batch 2:  2017-03-30 11:32:06.059012 Cost: 0.00008\tAccuracy: 0.71200\n",
      "Epoch 85, CIFAR-10 Batch 3:  2017-03-30 11:32:09.963936 Cost: 0.00127\tAccuracy: 0.70720\n",
      "Epoch 85, CIFAR-10 Batch 4:  2017-03-30 11:32:13.865129 Cost: 0.00037\tAccuracy: 0.71500\n",
      "Epoch 85, CIFAR-10 Batch 5:  2017-03-30 11:32:17.761900 Cost: 0.00002\tAccuracy: 0.71300\n",
      "Epoch 86, CIFAR-10 Batch 1:  2017-03-30 11:32:21.664537 Cost: 0.00001\tAccuracy: 0.71460\n",
      "Epoch 86, CIFAR-10 Batch 2:  2017-03-30 11:32:25.567718 Cost: 0.00013\tAccuracy: 0.71140\n",
      "Epoch 86, CIFAR-10 Batch 3:  2017-03-30 11:32:29.467524 Cost: 0.00000\tAccuracy: 0.71080\n",
      "Epoch 86, CIFAR-10 Batch 4:  2017-03-30 11:32:33.371928 Cost: 0.00005\tAccuracy: 0.71500\n",
      "Epoch 86, CIFAR-10 Batch 5:  2017-03-30 11:32:37.271110 Cost: 0.00004\tAccuracy: 0.70920\n",
      "Epoch 87, CIFAR-10 Batch 1:  2017-03-30 11:32:41.174563 Cost: 0.00012\tAccuracy: 0.71320\n",
      "Epoch 87, CIFAR-10 Batch 2:  2017-03-30 11:32:45.073121 Cost: 0.00010\tAccuracy: 0.70360\n",
      "Epoch 87, CIFAR-10 Batch 3:  2017-03-30 11:32:48.980025 Cost: 0.00001\tAccuracy: 0.70860\n",
      "Epoch 87, CIFAR-10 Batch 4:  2017-03-30 11:32:52.878970 Cost: 0.00006\tAccuracy: 0.70660\n",
      "Epoch 87, CIFAR-10 Batch 5:  2017-03-30 11:32:56.782961 Cost: 0.00004\tAccuracy: 0.70400\n",
      "Epoch 88, CIFAR-10 Batch 1:  2017-03-30 11:33:00.684079 Cost: 0.00002\tAccuracy: 0.70600\n",
      "Epoch 88, CIFAR-10 Batch 2:  2017-03-30 11:33:04.592820 Cost: 0.00004\tAccuracy: 0.70600\n",
      "Epoch 88, CIFAR-10 Batch 3:  2017-03-30 11:33:08.510140 Cost: 0.00000\tAccuracy: 0.70700\n",
      "Epoch 88, CIFAR-10 Batch 4:  2017-03-30 11:33:12.420603 Cost: 0.00002\tAccuracy: 0.71200\n",
      "Epoch 88, CIFAR-10 Batch 5:  2017-03-30 11:33:16.325391 Cost: 0.00000\tAccuracy: 0.71420\n",
      "Epoch 89, CIFAR-10 Batch 1:  2017-03-30 11:33:20.227538 Cost: 0.00000\tAccuracy: 0.71660\n",
      "Epoch 89, CIFAR-10 Batch 2:  2017-03-30 11:33:24.128978 Cost: 0.00045\tAccuracy: 0.70700\n",
      "Epoch 89, CIFAR-10 Batch 3:  2017-03-30 11:33:28.032332 Cost: 0.00000\tAccuracy: 0.71160\n",
      "Epoch 89, CIFAR-10 Batch 4:  2017-03-30 11:33:31.929685 Cost: 0.00007\tAccuracy: 0.71240\n",
      "Epoch 89, CIFAR-10 Batch 5:  2017-03-30 11:33:35.823899 Cost: 0.00001\tAccuracy: 0.71360\n",
      "Epoch 90, CIFAR-10 Batch 1:  2017-03-30 11:33:39.727349 Cost: 0.00000\tAccuracy: 0.71640\n",
      "Epoch 90, CIFAR-10 Batch 2:  2017-03-30 11:33:43.637570 Cost: 0.00017\tAccuracy: 0.70780\n",
      "Epoch 90, CIFAR-10 Batch 3:  2017-03-30 11:33:47.540735 Cost: 0.00000\tAccuracy: 0.71820\n",
      "Epoch 90, CIFAR-10 Batch 4:  2017-03-30 11:33:51.439879 Cost: 0.00002\tAccuracy: 0.71580\n",
      "Epoch 90, CIFAR-10 Batch 5:  2017-03-30 11:33:55.335688 Cost: 0.00000\tAccuracy: 0.71140\n",
      "Epoch 91, CIFAR-10 Batch 1:  2017-03-30 11:33:59.239923 Cost: 0.00001\tAccuracy: 0.72200\n",
      "Epoch 91, CIFAR-10 Batch 2:  2017-03-30 11:34:03.145928 Cost: 0.00005\tAccuracy: 0.71460\n",
      "Epoch 91, CIFAR-10 Batch 3:  2017-03-30 11:34:07.053799 Cost: 0.00001\tAccuracy: 0.71540\n",
      "Epoch 91, CIFAR-10 Batch 4:  2017-03-30 11:34:10.954665 Cost: 0.00001\tAccuracy: 0.70900\n",
      "Epoch 91, CIFAR-10 Batch 5:  2017-03-30 11:34:14.851660 Cost: 0.00089\tAccuracy: 0.71680\n",
      "Epoch 92, CIFAR-10 Batch 1:  2017-03-30 11:34:18.752839 Cost: 0.00002\tAccuracy: 0.71360\n",
      "Epoch 92, CIFAR-10 Batch 2:  2017-03-30 11:34:22.653181 Cost: 0.00008\tAccuracy: 0.70680\n",
      "Epoch 92, CIFAR-10 Batch 3:  2017-03-30 11:34:26.555312 Cost: 0.00001\tAccuracy: 0.71220\n",
      "Epoch 92, CIFAR-10 Batch 4:  2017-03-30 11:34:30.457345 Cost: 0.00001\tAccuracy: 0.71860\n",
      "Epoch 92, CIFAR-10 Batch 5:  2017-03-30 11:34:34.353066 Cost: 0.00001\tAccuracy: 0.71320\n",
      "Epoch 93, CIFAR-10 Batch 1:  2017-03-30 11:34:38.254441 Cost: 0.00002\tAccuracy: 0.71920\n",
      "Epoch 93, CIFAR-10 Batch 2:  2017-03-30 11:34:42.152534 Cost: 0.00002\tAccuracy: 0.71140\n",
      "Epoch 93, CIFAR-10 Batch 3:  2017-03-30 11:34:46.052670 Cost: 0.00001\tAccuracy: 0.70360\n",
      "Epoch 93, CIFAR-10 Batch 4:  2017-03-30 11:34:49.952144 Cost: 0.00003\tAccuracy: 0.71600\n",
      "Epoch 93, CIFAR-10 Batch 5:  2017-03-30 11:34:53.849137 Cost: 0.00006\tAccuracy: 0.72360\n",
      "Epoch 94, CIFAR-10 Batch 1:  2017-03-30 11:34:57.752842 Cost: 0.00001\tAccuracy: 0.72160\n",
      "Epoch 94, CIFAR-10 Batch 2:  2017-03-30 11:35:01.656155 Cost: 0.00010\tAccuracy: 0.71440\n",
      "Epoch 94, CIFAR-10 Batch 3:  2017-03-30 11:35:05.564902 Cost: 0.00000\tAccuracy: 0.71620\n",
      "Epoch 94, CIFAR-10 Batch 4:  2017-03-30 11:35:09.463203 Cost: 0.00001\tAccuracy: 0.71260\n",
      "Epoch 94, CIFAR-10 Batch 5:  2017-03-30 11:35:13.384042 Cost: 0.00003\tAccuracy: 0.71640\n",
      "Epoch 95, CIFAR-10 Batch 1:  2017-03-30 11:35:17.316550 Cost: 0.00007\tAccuracy: 0.71640\n",
      "Epoch 95, CIFAR-10 Batch 2:  2017-03-30 11:35:21.247409 Cost: 0.00002\tAccuracy: 0.71560\n",
      "Epoch 95, CIFAR-10 Batch 3:  2017-03-30 11:35:25.160528 Cost: 0.00000\tAccuracy: 0.71120\n",
      "Epoch 95, CIFAR-10 Batch 4:  2017-03-30 11:35:29.060739 Cost: 0.00003\tAccuracy: 0.71660\n",
      "Epoch 95, CIFAR-10 Batch 5:  2017-03-30 11:35:32.963009 Cost: 0.00007\tAccuracy: 0.71360\n",
      "Epoch 96, CIFAR-10 Batch 1:  2017-03-30 11:35:36.868600 Cost: 0.00001\tAccuracy: 0.71860\n",
      "Epoch 96, CIFAR-10 Batch 2:  2017-03-30 11:35:40.775357 Cost: 0.00002\tAccuracy: 0.72180\n",
      "Epoch 96, CIFAR-10 Batch 3:  2017-03-30 11:35:44.676156 Cost: 0.00000\tAccuracy: 0.71620\n",
      "Epoch 96, CIFAR-10 Batch 4:  2017-03-30 11:35:48.574916 Cost: 0.00000\tAccuracy: 0.71820\n",
      "Epoch 96, CIFAR-10 Batch 5:  2017-03-30 11:35:52.470289 Cost: 0.00001\tAccuracy: 0.71960\n",
      "Epoch 97, CIFAR-10 Batch 1:  2017-03-30 11:35:56.370436 Cost: 0.00004\tAccuracy: 0.71300\n",
      "Epoch 97, CIFAR-10 Batch 2:  2017-03-30 11:36:00.269205 Cost: 0.00078\tAccuracy: 0.71600\n",
      "Epoch 97, CIFAR-10 Batch 3:  2017-03-30 11:36:04.175266 Cost: 0.00000\tAccuracy: 0.70600\n",
      "Epoch 97, CIFAR-10 Batch 4:  2017-03-30 11:36:08.076513 Cost: 0.00001\tAccuracy: 0.71460\n",
      "Epoch 97, CIFAR-10 Batch 5:  2017-03-30 11:36:11.974735 Cost: 0.00069\tAccuracy: 0.71540\n",
      "Epoch 98, CIFAR-10 Batch 1:  2017-03-30 11:36:15.875772 Cost: 0.00000\tAccuracy: 0.71340\n",
      "Epoch 98, CIFAR-10 Batch 2:  2017-03-30 11:36:19.778439 Cost: 0.00007\tAccuracy: 0.71780\n",
      "Epoch 98, CIFAR-10 Batch 3:  2017-03-30 11:36:23.676787 Cost: 0.00000\tAccuracy: 0.71520\n",
      "Epoch 98, CIFAR-10 Batch 4:  2017-03-30 11:36:27.578876 Cost: 0.00001\tAccuracy: 0.71600\n",
      "Epoch 98, CIFAR-10 Batch 5:  2017-03-30 11:36:31.476756 Cost: 0.00015\tAccuracy: 0.71700\n",
      "Epoch 99, CIFAR-10 Batch 1:  2017-03-30 11:36:35.378003 Cost: 0.00001\tAccuracy: 0.71160\n",
      "Epoch 99, CIFAR-10 Batch 2:  2017-03-30 11:36:39.288949 Cost: 0.00008\tAccuracy: 0.71400\n",
      "Epoch 99, CIFAR-10 Batch 3:  2017-03-30 11:36:43.191287 Cost: 0.00001\tAccuracy: 0.71080\n",
      "Epoch 99, CIFAR-10 Batch 4:  2017-03-30 11:36:47.089140 Cost: 0.00001\tAccuracy: 0.70840\n",
      "Epoch 99, CIFAR-10 Batch 5:  2017-03-30 11:36:50.988651 Cost: 0.00004\tAccuracy: 0.71260\n",
      "Epoch 100, CIFAR-10 Batch 1:  2017-03-30 11:36:54.888302 Cost: 0.00001\tAccuracy: 0.70780\n",
      "Epoch 100, CIFAR-10 Batch 2:  2017-03-30 11:36:58.788761 Cost: 0.00004\tAccuracy: 0.71960\n",
      "Epoch 100, CIFAR-10 Batch 3:  2017-03-30 11:37:02.697976 Cost: 0.00000\tAccuracy: 0.72140\n",
      "Epoch 100, CIFAR-10 Batch 4:  2017-03-30 11:37:06.595039 Cost: 0.00002\tAccuracy: 0.71740\n",
      "Epoch 100, CIFAR-10 Batch 5:  2017-03-30 11:37:10.492483 Cost: 0.00280\tAccuracy: 0.71740\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7087890625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0uRYRERERCT6/1AY7REuueUPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f84aef98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "495px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
